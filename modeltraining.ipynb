{"cells":[{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1689007806540,"execution_millis":13,"deepnote_to_be_reexecuted":false,"cell_id":"07e449b097b14ea3a61b5a999f044340","deepnote_cell_type":"code"},"source":"import nltk\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer=WordNetLemmatizer()\n\nimport json\nimport pickle\nimport numpy as np\nimport pandas as pd\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Activation,Dropout\nfrom keras.optimizers import SGD\n\nimport random","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1689007809038,"execution_millis":13,"deepnote_to_be_reexecuted":false,"cell_id":"12916616bb1d40dd9d14fc003b9ef37f","deepnote_cell_type":"code"},"source":"words=[]\nclasses=[]\ndocuments=[]\nignore_wrods=[]\ndata_file=open('intents.json')\nintents=json.load(data_file)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1689007811134,"execution_millis":20,"deepnote_to_be_reexecuted":false,"cell_id":"57cecbdb9b4341f0b2bf52c70fbf52f1","deepnote_cell_type":"code"},"source":"for intent in intents['intents']:\n    for patttern in intent['patterns']:\n        w=nltk.word_tokenize(patttern)\n        documents.append((w,intent['tag']))\n        \n        if intent['tag'] not in classes:\n            classes.append(intent['tag'])","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1689007813050,"execution_millis":53,"deepnote_to_be_reexecuted":false,"cell_id":"4c890953e35249bd895e0aa8dd3b85a2","deepnote_cell_type":"code"},"source":"words=[lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_wrods]\nwords=sorted(list(set(words)))\n\nclasses=sorted(list(set(classes)))\n\nprint(len(documents),\"docuements\")","execution_count":null,"outputs":[{"name":"stdout","text":"47 docuements\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1689007829292,"execution_millis":29,"deepnote_to_be_reexecuted":false,"cell_id":"f44d963981fa4ff59808dc31654a4bca","deepnote_cell_type":"code"},"source":"print(len(classes),\"classes\")","execution_count":null,"outputs":[{"name":"stdout","text":"9 classes\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1689007829300,"execution_millis":55,"deepnote_to_be_reexecuted":false,"cell_id":"abf397af15ab4804a893b3e1e5e369dc","deepnote_cell_type":"code"},"source":"pickle.dump(words,open(\"words.pkl\",'wb'))\npickle.dump(classes,open(\"classes.pkl\",'wb'))","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"source_hash":null,"execution_start":1689007829390,"execution_millis":168,"deepnote_to_be_reexecuted":false,"cell_id":"c23862ebc5784cc39cacc1639fe6a2e7","deepnote_cell_type":"code"},"source":"trainig=[]\n\noutput_empty=[0]*len(classes)\n\nfor doc in documents:\n    bag=[]\n    patttern_words=doc[0]\n    patttern_words=[lemmatizer.lemmatize(word.lower()) for word in patttern_words]\n    \n    print(patttern_words)\n    \n    for w in words:\n        bag.append(1) if w in patttern_words else bag.append(0)\n        \n    output_row=list(output_empty)\n    output_row[classes.index(doc[1])] =1\n    \n    trainig.append([bag,output_row])\n","execution_count":null,"outputs":[{"name":"stdout","text":"['hi', 'there']\n['how', 'are', 'you']\n['is', 'anyone', 'there', '?']\n['hey']\n['hola']\n['hello']\n['good', 'day']\n['bye']\n['see', 'you', 'later']\n['goodbye']\n['nice', 'chatting', 'to', 'you', ',', 'bye']\n['till', 'next', 'time']\n['thanks']\n['thank', 'you']\n['that', \"'s\", 'helpful']\n['awesome', ',', 'thanks']\n['thanks', 'for', 'helping', 'me']\n['how', 'you', 'could', 'help', 'me', '?']\n['what', 'you', 'can', 'do', '?']\n['what', 'help', 'you', 'provide', '?']\n['how', 'you', 'can', 'be', 'helpful', '?']\n['what', 'support', 'is', 'offered']\n['how', 'to', 'check', 'adverse', 'drug', 'reaction', '?']\n['open', 'adverse', 'drug', 'module']\n['give', 'me', 'a', 'list', 'of', 'drug', 'causing', 'adverse', 'behavior']\n['list', 'all', 'drug', 'suitable', 'for', 'patient', 'with', 'adverse', 'reaction']\n['which', 'drug', 'dont', 'have', 'adverse', 'reaction', '?']\n['open', 'blood', 'pressure', 'module']\n['task', 'related', 'to', 'blood', 'pressure']\n['blood', 'pressure', 'data', 'entry']\n['i', 'want', 'to', 'log', 'blood', 'pressure', 'result']\n['blood', 'pressure', 'data', 'management']\n['i', 'want', 'to', 'search', 'for', 'blood', 'pressure', 'result', 'history']\n['blood', 'pressure', 'for', 'patient']\n['load', 'patient', 'blood', 'pressure', 'result']\n['show', 'blood', 'pressure', 'result', 'for', 'patient']\n['find', 'blood', 'pressure', 'result', 'by', 'id']\n['find', 'me', 'a', 'pharmacy']\n['find', 'pharmacy']\n['list', 'of', 'pharmacy', 'nearby']\n['locate', 'pharmacy']\n['search', 'pharmacy']\n['lookup', 'for', 'hospital']\n['searching', 'for', 'hospital', 'to', 'transfer', 'patient']\n['i', 'want', 'to', 'search', 'hospital', 'data']\n['hospital', 'lookup', 'for', 'patient']\n['looking', 'up', 'hospital', 'detail']\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1689007829411,"execution_millis":65,"deepnote_to_be_reexecuted":false,"cell_id":"34eeb4fc428e415cb58aae05a8f265f0","deepnote_cell_type":"code"},"source":"random.shuffle(trainig)\ntrainig=np.array(trainig)\n# print(trainig)","execution_count":null,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_108/1912861731.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n  trainig=np.array(trainig)\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1689008465270,"execution_millis":29,"deepnote_to_be_reexecuted":false,"cell_id":"a87b06457b4a43c5ad04c48ee9f476e8","deepnote_cell_type":"code"},"source":"train_x = np.array([x for x, _ in trainig])\n\ntrain_y = np.array([y for _, y in trainig])\ntrain_x = train_x.tolist()\ntrain_y = train_y.tolist()\n\n\nprint(\"traing data created\")","execution_count":null,"outputs":[{"name":"stdout","text":"traing data created\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1689008468177,"execution_millis":74,"deepnote_to_be_reexecuted":false,"cell_id":"71dbf9edc7ec45c7b824bbd54b5d2451","deepnote_cell_type":"code"},"source":"model=Sequential()\nmodel.add(Dense(128,input_shape=(len(train_x[0]),),activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64,activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(len(train_y[0]),activation='softmax'))","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1689008470964,"execution_millis":10,"deepnote_to_be_reexecuted":false,"cell_id":"f5b5da5698fd49428c4d2e2c30d1ddae","deepnote_cell_type":"code"},"source":"sgd=SGD(learning_rate=0.01,decay=1e-6,momentum=0.9,nesterov=True)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1689008472549,"execution_millis":44,"deepnote_to_be_reexecuted":false,"cell_id":"cc6c8a81f2784fc7b9434b6d925e0a96","deepnote_cell_type":"code"},"source":"model.compile(loss='categotical_cross_entropy',optimizer=sgd,metrics=['accuracy'])","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1689008478976,"execution_millis":235,"deepnote_to_be_reexecuted":false,"cell_id":"106c1ff58e51437fb95b4f6784b2d1b6","deepnote_cell_type":"code"},"source":"history = model.fit(train_x, train_y, epochs=200, batch_size=5, verbose=1)\nmodel.save('chatbot_model.h5')\nhistory = model.fit(train_x, train_y, epochs=200, batch_size=5, verbose=1)\nmodel.save('chatbot_model.h5')\n","execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"(<class 'list'> containing values of types set())\"}), (<class 'list'> containing values of types {'(<class \\'list\\'> containing values of types {\"<class \\'int\\'>\"})'})","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn [50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchatbot_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(train_x, train_y, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/shared-libs/python3.9/py/lib/python3.9/site-packages/keras/engine/data_adapter.py:1083\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1080\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mcls\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ALL_ADAPTER_CLS \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mcan_handle(x, y)]\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m adapter_cls:\n\u001b[1;32m   1082\u001b[0m     \u001b[38;5;66;03m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[0;32m-> 1083\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1084\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to find data adapter that can handle \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1085\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(_type_name(x), _type_name(y))\n\u001b[1;32m   1086\u001b[0m     )\n\u001b[1;32m   1087\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(adapter_cls) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1088\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1089\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData adapters should be mutually exclusive for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1090\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhandling inputs. Found multiple adapters \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m to handle \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1091\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(adapter_cls, _type_name(x), _type_name(y))\n\u001b[1;32m   1092\u001b[0m     )\n","\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"(<class 'list'> containing values of types set())\"}), (<class 'list'> containing values of types {'(<class \\'list\\'> containing values of types {\"<class \\'int\\'>\"})'})"]}]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1689008534302,"execution_millis":8138,"deepnote_to_be_reexecuted":false,"cell_id":"0e48c8c9a2974283804a84e9b5884032","deepnote_cell_type":"code"},"source":"import nltk\nfrom nltk.stem import WordNetLemmatizer\nimport json\nimport pickle\nimport numpy as np\nimport pandas as pd\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nfrom keras.optimizers import SGD\n\nimport random\n\nwords = []\nclasses = []\ndocuments = []\nignore_words = []\n\ndata_file = open('intents.json')\nintents = json.load(data_file)\n\nlemmatizer = WordNetLemmatizer()\n\nfor intent in intents['intents']:\n    for pattern in intent['patterns']:\n        w = nltk.word_tokenize(pattern)\n        documents.append((w, intent['tag']))\n\n        if intent['tag'] not in classes:\n            classes.append(intent['tag'])\n\nwords = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\nwords = sorted(list(set(words)))\nclasses = sorted(list(set(classes)))\n\nprint(len(documents), \"documents\")\nprint(len(classes), \"classes\")\n\npickle.dump(words, open(\"words.pkl\", 'wb'))\npickle.dump(classes, open(\"classes.pkl\", 'wb'))\n\ntraining = []\n\noutput_empty = [0] * len(classes)\n\nfor doc in documents:\n    bag = []\n    pattern_words = doc[0]\n    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n\n    for w in words:\n        bag.append(1) if w in pattern_words else bag.append(0)\n\n    output_row = list(output_empty)\n    output_row[classes.index(doc[1])] = 1\n\n    training.append([bag, output_row])\n\nrandom.shuffle(training)\n\ntrain_x = []\ntrain_y = []\n\nfor feature, label in training:\n    train_x.append(feature)\n    train_y.append(label)\n\nprint(\"Training data created\")\n\ntrain_x = np.array(train_x)\ntrain_y = np.array(train_y)\n\nmodel = Sequential()\nmodel.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(len(train_y[0]), activation='softmax'))\n\nsgd = SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n\nhistory = model.fit(train_x, train_y, epochs=200, batch_size=5, verbose=1)\n\nmodel.save('chatbot_model.h5')\n","execution_count":null,"outputs":[{"name":"stdout","text":"47 documents\n9 classes\nTraining data created\nEpoch 1/200\n10/10 [==============================] - 1s 4ms/step - loss: 2.1993 - accuracy: 0.0851\nEpoch 2/200\n10/10 [==============================] - 0s 7ms/step - loss: 2.1981 - accuracy: 0.1489\nEpoch 3/200\n10/10 [==============================] - 0s 3ms/step - loss: 2.1970 - accuracy: 0.1489\nEpoch 4/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1961 - accuracy: 0.1489\nEpoch 5/200\n10/10 [==============================] - 0s 1ms/step - loss: 2.1969 - accuracy: 0.1489\nEpoch 6/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1950 - accuracy: 0.1489\nEpoch 7/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1940 - accuracy: 0.1489\nEpoch 8/200\n10/10 [==============================] - 0s 6ms/step - loss: 2.1956 - accuracy: 0.1489\nEpoch 9/200\n10/10 [==============================] - 0s 4ms/step - loss: 2.1941 - accuracy: 0.1489\nEpoch 10/200\n10/10 [==============================] - 0s 4ms/step - loss: 2.1942 - accuracy: 0.1489\nEpoch 11/200\n10/10 [==============================] - 0s 9ms/step - loss: 2.1936 - accuracy: 0.1489\nEpoch 12/200\n10/10 [==============================] - 0s 1ms/step - loss: 2.1941 - accuracy: 0.1489\nEpoch 13/200\n10/10 [==============================] - 0s 1ms/step - loss: 2.1944 - accuracy: 0.1489\nEpoch 14/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1941 - accuracy: 0.1489\nEpoch 15/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1942 - accuracy: 0.1489\nEpoch 16/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1936 - accuracy: 0.1489\nEpoch 17/200\n10/10 [==============================] - 0s 3ms/step - loss: 2.1927 - accuracy: 0.1489\nEpoch 18/200\n10/10 [==============================] - 0s 1ms/step - loss: 2.1949 - accuracy: 0.1489\nEpoch 19/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1945 - accuracy: 0.1489\nEpoch 20/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1931 - accuracy: 0.1489\nEpoch 21/200\n10/10 [==============================] - 0s 3ms/step - loss: 2.1935 - accuracy: 0.1489\nEpoch 22/200\n10/10 [==============================] - 0s 3ms/step - loss: 2.1943 - accuracy: 0.1489\nEpoch 23/200\n10/10 [==============================] - 0s 10ms/step - loss: 2.1945 - accuracy: 0.1489\nEpoch 24/200\n10/10 [==============================] - 0s 4ms/step - loss: 2.1929 - accuracy: 0.1489\nEpoch 25/200\n10/10 [==============================] - 0s 3ms/step - loss: 2.1938 - accuracy: 0.1489\nEpoch 26/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1936 - accuracy: 0.1489\nEpoch 27/200\n10/10 [==============================] - 0s 3ms/step - loss: 2.1928 - accuracy: 0.1489\nEpoch 28/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1937 - accuracy: 0.1489\nEpoch 29/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1933 - accuracy: 0.1489\nEpoch 30/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1930 - accuracy: 0.1489\nEpoch 31/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1942 - accuracy: 0.1489\nEpoch 32/200\n10/10 [==============================] - 0s 7ms/step - loss: 2.1926 - accuracy: 0.1489\nEpoch 33/200\n10/10 [==============================] - 0s 3ms/step - loss: 2.1940 - accuracy: 0.1489\nEpoch 34/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1953 - accuracy: 0.1489\nEpoch 35/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1930 - accuracy: 0.1489\nEpoch 36/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1939 - accuracy: 0.1489\nEpoch 37/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1932 - accuracy: 0.1489\nEpoch 38/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1937 - accuracy: 0.1489\nEpoch 39/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1942 - accuracy: 0.1489\nEpoch 40/200\n10/10 [==============================] - 0s 4ms/step - loss: 2.1932 - accuracy: 0.1489\nEpoch 41/200\n10/10 [==============================] - 0s 4ms/step - loss: 2.1949 - accuracy: 0.1489\nEpoch 42/200\n10/10 [==============================] - 0s 4ms/step - loss: 2.1940 - accuracy: 0.1489\nEpoch 43/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1936 - accuracy: 0.1489\nEpoch 44/200\n10/10 [==============================] - 0s 1ms/step - loss: 2.1925 - accuracy: 0.1489\nEpoch 45/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1932 - accuracy: 0.1489\nEpoch 46/200\n10/10 [==============================] - 0s 4ms/step - loss: 2.1937 - accuracy: 0.1489\nEpoch 47/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1932 - accuracy: 0.1489\nEpoch 48/200\n10/10 [==============================] - 0s 8ms/step - loss: 2.1942 - accuracy: 0.1489\nEpoch 49/200\n10/10 [==============================] - 0s 6ms/step - loss: 2.1937 - accuracy: 0.1489\nEpoch 50/200\n10/10 [==============================] - 0s 4ms/step - loss: 2.1933 - accuracy: 0.1489\nEpoch 51/200\n10/10 [==============================] - 0s 6ms/step - loss: 2.1934 - accuracy: 0.1489\nEpoch 52/200\n10/10 [==============================] - 0s 4ms/step - loss: 2.1945 - accuracy: 0.1489\nEpoch 53/200\n10/10 [==============================] - 0s 4ms/step - loss: 2.1945 - accuracy: 0.1489\nEpoch 54/200\n10/10 [==============================] - 0s 5ms/step - loss: 2.1935 - accuracy: 0.1489\nEpoch 55/200\n10/10 [==============================] - 0s 3ms/step - loss: 2.1933 - accuracy: 0.1489\nEpoch 56/200\n10/10 [==============================] - 0s 4ms/step - loss: 2.1949 - accuracy: 0.1489\nEpoch 57/200\n10/10 [==============================] - 0s 1ms/step - loss: 2.1943 - accuracy: 0.1489\nEpoch 58/200\n10/10 [==============================] - 0s 3ms/step - loss: 2.1943 - accuracy: 0.1489\nEpoch 59/200\n10/10 [==============================] - 0s 1ms/step - loss: 2.1940 - accuracy: 0.1489\nEpoch 60/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1930 - accuracy: 0.1489\nEpoch 61/200\n10/10 [==============================] - 0s 1ms/step - loss: 2.1957 - accuracy: 0.1489\nEpoch 62/200\n10/10 [==============================] - 0s 1ms/step - loss: 2.1930 - accuracy: 0.1489\nEpoch 63/200\n10/10 [==============================] - 0s 1ms/step - loss: 2.1936 - accuracy: 0.1489\nEpoch 64/200\n10/10 [==============================] - 0s 6ms/step - loss: 2.1942 - accuracy: 0.1489\nEpoch 65/200\n10/10 [==============================] - 0s 3ms/step - loss: 2.1944 - accuracy: 0.1489\nEpoch 66/200\n10/10 [==============================] - 0s 3ms/step - loss: 2.1949 - accuracy: 0.1489\nEpoch 67/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1946 - accuracy: 0.1489\nEpoch 68/200\n10/10 [==============================] - 0s 3ms/step - loss: 2.1939 - accuracy: 0.1489\nEpoch 69/200\n10/10 [==============================] - 0s 3ms/step - loss: 2.1932 - accuracy: 0.1489\nEpoch 70/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1947 - accuracy: 0.1489\nEpoch 71/200\n10/10 [==============================] - 0s 4ms/step - loss: 2.1946 - accuracy: 0.1489\nEpoch 72/200\n10/10 [==============================] - 0s 5ms/step - loss: 2.1943 - accuracy: 0.1489\nEpoch 73/200\n10/10 [==============================] - 0s 5ms/step - loss: 2.1939 - accuracy: 0.1489\nEpoch 74/200\n10/10 [==============================] - 0s 4ms/step - loss: 2.1936 - accuracy: 0.1489\nEpoch 75/200\n10/10 [==============================] - 0s 3ms/step - loss: 2.1940 - accuracy: 0.1489\nEpoch 76/200\n10/10 [==============================] - 0s 11ms/step - loss: 2.1955 - accuracy: 0.1489\nEpoch 77/200\n10/10 [==============================] - 0s 7ms/step - loss: 2.1949 - accuracy: 0.1489\nEpoch 78/200\n10/10 [==============================] - 0s 3ms/step - loss: 2.1936 - accuracy: 0.1489\nEpoch 79/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1957 - accuracy: 0.1489\nEpoch 80/200\n10/10 [==============================] - 0s 1ms/step - loss: 2.1955 - accuracy: 0.1489\nEpoch 81/200\n10/10 [==============================] - 0s 3ms/step - loss: 2.1959 - accuracy: 0.1489\nEpoch 82/200\n10/10 [==============================] - 0s 4ms/step - loss: 2.1937 - accuracy: 0.1489\nEpoch 83/200\n10/10 [==============================] - 0s 4ms/step - loss: 2.1935 - accuracy: 0.1489\nEpoch 84/200\n10/10 [==============================] - 0s 7ms/step - loss: 2.1950 - accuracy: 0.1489\nEpoch 85/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1940 - accuracy: 0.1489\nEpoch 86/200\n10/10 [==============================] - 0s 1ms/step - loss: 2.1946 - accuracy: 0.1489\nEpoch 87/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1934 - accuracy: 0.1489\nEpoch 88/200\n10/10 [==============================] - 0s 1ms/step - loss: 2.1925 - accuracy: 0.1489\nEpoch 89/200\n10/10 [==============================] - 0s 1ms/step - loss: 2.1936 - accuracy: 0.1489\nEpoch 90/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1950 - accuracy: 0.1489\nEpoch 91/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1960 - accuracy: 0.1489\nEpoch 92/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1944 - accuracy: 0.1489\nEpoch 93/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1943 - accuracy: 0.1489\nEpoch 94/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1942 - accuracy: 0.1489\nEpoch 95/200\n10/10 [==============================] - 0s 4ms/step - loss: 2.1946 - accuracy: 0.1489\nEpoch 96/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1943 - accuracy: 0.1489\nEpoch 97/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1952 - accuracy: 0.1489\nEpoch 98/200\n10/10 [==============================] - 0s 4ms/step - loss: 2.1945 - accuracy: 0.1489\nEpoch 99/200\n10/10 [==============================] - 0s 11ms/step - loss: 2.1926 - accuracy: 0.1489\nEpoch 100/200\n10/10 [==============================] - 0s 5ms/step - loss: 2.1949 - accuracy: 0.1489\nEpoch 101/200\n10/10 [==============================] - 0s 3ms/step - loss: 2.1944 - accuracy: 0.1489\nEpoch 102/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1937 - accuracy: 0.1489\nEpoch 103/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1952 - accuracy: 0.1489\nEpoch 104/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1947 - accuracy: 0.1489\nEpoch 105/200\n10/10 [==============================] - 0s 3ms/step - loss: 2.1948 - accuracy: 0.1489\nEpoch 106/200\n10/10 [==============================] - 0s 4ms/step - loss: 2.1937 - accuracy: 0.1489\nEpoch 107/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1948 - accuracy: 0.1489\nEpoch 108/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1943 - accuracy: 0.1489\nEpoch 109/200\n10/10 [==============================] - 0s 1ms/step - loss: 2.1944 - accuracy: 0.1489\nEpoch 110/200\n10/10 [==============================] - 0s 6ms/step - loss: 2.1947 - accuracy: 0.1489\nEpoch 111/200\n10/10 [==============================] - 0s 3ms/step - loss: 2.1944 - accuracy: 0.1489\nEpoch 112/200\n10/10 [==============================] - 0s 1ms/step - loss: 2.1939 - accuracy: 0.1489\nEpoch 113/200\n10/10 [==============================] - 0s 1ms/step - loss: 2.1934 - accuracy: 0.1489\nEpoch 114/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1927 - accuracy: 0.1489\nEpoch 115/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1953 - accuracy: 0.1489\nEpoch 116/200\n10/10 [==============================] - 0s 4ms/step - loss: 2.1934 - accuracy: 0.1489\nEpoch 117/200\n10/10 [==============================] - 0s 8ms/step - loss: 2.1956 - accuracy: 0.1489\nEpoch 118/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1937 - accuracy: 0.1489\nEpoch 119/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1937 - accuracy: 0.1489\nEpoch 120/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1942 - accuracy: 0.1489\nEpoch 121/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1934 - accuracy: 0.1489\nEpoch 122/200\n10/10 [==============================] - 0s 4ms/step - loss: 2.1936 - accuracy: 0.1489\nEpoch 123/200\n10/10 [==============================] - 0s 3ms/step - loss: 2.1936 - accuracy: 0.1489\nEpoch 124/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1930 - accuracy: 0.1489\nEpoch 125/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1943 - accuracy: 0.1489\nEpoch 126/200\n10/10 [==============================] - 0s 1ms/step - loss: 2.1937 - accuracy: 0.1489\nEpoch 127/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1925 - accuracy: 0.1489\nEpoch 128/200\n10/10 [==============================] - 0s 4ms/step - loss: 2.1933 - accuracy: 0.1489\nEpoch 129/200\n10/10 [==============================] - 0s 4ms/step - loss: 2.1948 - accuracy: 0.1489\nEpoch 130/200\n10/10 [==============================] - 0s 5ms/step - loss: 2.1929 - accuracy: 0.1489\nEpoch 131/200\n10/10 [==============================] - 0s 3ms/step - loss: 2.1945 - accuracy: 0.1489\nEpoch 132/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1950 - accuracy: 0.1489\nEpoch 133/200\n10/10 [==============================] - 0s 3ms/step - loss: 2.1940 - accuracy: 0.1489\nEpoch 134/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1932 - accuracy: 0.1489\nEpoch 135/200\n10/10 [==============================] - 0s 3ms/step - loss: 2.1951 - accuracy: 0.1489\nEpoch 136/200\n10/10 [==============================] - 0s 3ms/step - loss: 2.1933 - accuracy: 0.1489\nEpoch 137/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1939 - accuracy: 0.1489\nEpoch 138/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1932 - accuracy: 0.1489\nEpoch 139/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1933 - accuracy: 0.1489\nEpoch 140/200\n10/10 [==============================] - 0s 3ms/step - loss: 2.1937 - accuracy: 0.1489\nEpoch 141/200\n10/10 [==============================] - 0s 3ms/step - loss: 2.1933 - accuracy: 0.1489\nEpoch 142/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1930 - accuracy: 0.1489\nEpoch 143/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1954 - accuracy: 0.1489\nEpoch 144/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1926 - accuracy: 0.1489\nEpoch 145/200\n10/10 [==============================] - 0s 1ms/step - loss: 2.1939 - accuracy: 0.1489\nEpoch 146/200\n10/10 [==============================] - 0s 1ms/step - loss: 2.1934 - accuracy: 0.1489\nEpoch 147/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1942 - accuracy: 0.1489\nEpoch 148/200\n10/10 [==============================] - 0s 1ms/step - loss: 2.1960 - accuracy: 0.1489\nEpoch 149/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1930 - accuracy: 0.1489\nEpoch 150/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1945 - accuracy: 0.1489\nEpoch 151/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1942 - accuracy: 0.1489\nEpoch 152/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1958 - accuracy: 0.1489\nEpoch 153/200\n10/10 [==============================] - 0s 1ms/step - loss: 2.1945 - accuracy: 0.1489\nEpoch 154/200\n10/10 [==============================] - 0s 1ms/step - loss: 2.1944 - accuracy: 0.1489\nEpoch 155/200\n10/10 [==============================] - 0s 1ms/step - loss: 2.1933 - accuracy: 0.1489\nEpoch 156/200\n10/10 [==============================] - 0s 1ms/step - loss: 2.1941 - accuracy: 0.1489\nEpoch 157/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1948 - accuracy: 0.1489\nEpoch 158/200\n10/10 [==============================] - 0s 7ms/step - loss: 2.1933 - accuracy: 0.1489\nEpoch 159/200\n10/10 [==============================] - 0s 4ms/step - loss: 2.1931 - accuracy: 0.1489\nEpoch 160/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1939 - accuracy: 0.1489\nEpoch 161/200\n10/10 [==============================] - 0s 5ms/step - loss: 2.1934 - accuracy: 0.1489\nEpoch 162/200\n10/10 [==============================] - 0s 7ms/step - loss: 2.1940 - accuracy: 0.1489\nEpoch 163/200\n10/10 [==============================] - 0s 10ms/step - loss: 2.1932 - accuracy: 0.1489\nEpoch 164/200\n10/10 [==============================] - 0s 3ms/step - loss: 2.1930 - accuracy: 0.1489\nEpoch 165/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1945 - accuracy: 0.1489\nEpoch 166/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1934 - accuracy: 0.1489\nEpoch 167/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1946 - accuracy: 0.1489\nEpoch 168/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1932 - accuracy: 0.1489\nEpoch 169/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1929 - accuracy: 0.1489\nEpoch 170/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1935 - accuracy: 0.1489\nEpoch 171/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1931 - accuracy: 0.1489\nEpoch 172/200\n10/10 [==============================] - 0s 3ms/step - loss: 2.1948 - accuracy: 0.1489\nEpoch 173/200\n10/10 [==============================] - 0s 5ms/step - loss: 2.1944 - accuracy: 0.1489\nEpoch 174/200\n10/10 [==============================] - 0s 5ms/step - loss: 2.1949 - accuracy: 0.1489\nEpoch 175/200\n10/10 [==============================] - 0s 4ms/step - loss: 2.1931 - accuracy: 0.1489\nEpoch 176/200\n10/10 [==============================] - 0s 4ms/step - loss: 2.1943 - accuracy: 0.1489\nEpoch 177/200\n10/10 [==============================] - 0s 4ms/step - loss: 2.1951 - accuracy: 0.1489\nEpoch 178/200\n10/10 [==============================] - 0s 4ms/step - loss: 2.1951 - accuracy: 0.1489\nEpoch 179/200\n10/10 [==============================] - 0s 3ms/step - loss: 2.1936 - accuracy: 0.1489\nEpoch 180/200\n10/10 [==============================] - 0s 1ms/step - loss: 2.1946 - accuracy: 0.1489\nEpoch 181/200\n10/10 [==============================] - 0s 3ms/step - loss: 2.1932 - accuracy: 0.1489\nEpoch 182/200\n10/10 [==============================] - 0s 4ms/step - loss: 2.1940 - accuracy: 0.1489\nEpoch 183/200\n10/10 [==============================] - 0s 5ms/step - loss: 2.1940 - accuracy: 0.1489\nEpoch 184/200\n10/10 [==============================] - 0s 7ms/step - loss: 2.1934 - accuracy: 0.1489\nEpoch 185/200\n10/10 [==============================] - 0s 5ms/step - loss: 2.1944 - accuracy: 0.1489\nEpoch 186/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1933 - accuracy: 0.1489\nEpoch 187/200\n10/10 [==============================] - 0s 1ms/step - loss: 2.1939 - accuracy: 0.1489\nEpoch 188/200\n10/10 [==============================] - 0s 3ms/step - loss: 2.1950 - accuracy: 0.1489\nEpoch 189/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1938 - accuracy: 0.1489\nEpoch 190/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1937 - accuracy: 0.1489\nEpoch 191/200\n10/10 [==============================] - 0s 3ms/step - loss: 2.1946 - accuracy: 0.1489\nEpoch 192/200\n10/10 [==============================] - 0s 5ms/step - loss: 2.1942 - accuracy: 0.1489\nEpoch 193/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1936 - accuracy: 0.1489\nEpoch 194/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1931 - accuracy: 0.1489\nEpoch 195/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1934 - accuracy: 0.1489\nEpoch 196/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1933 - accuracy: 0.1489\nEpoch 197/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1943 - accuracy: 0.1489\nEpoch 198/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1941 - accuracy: 0.1489\nEpoch 199/200\n10/10 [==============================] - 0s 3ms/step - loss: 2.1944 - accuracy: 0.1489\nEpoch 200/200\n10/10 [==============================] - 0s 2ms/step - loss: 2.1927 - accuracy: 0.1489\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=ca65c507-4a03-4f68-a58a-e0ae01abcbb4' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3 (ipykernel)"},"language_info":{"name":"python","version":"3.9.17","mimetype":"text/x-python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python"},"deepnote_notebook_id":"1b47ea6837074236914fbd13164537d5","deepnote_persisted_session":{"createdAt":"2023-07-10T17:19:34.819Z"},"deepnote_execution_queue":[]}}